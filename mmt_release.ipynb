{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y117dIimYVuW"
      },
      "source": [
        "This Colab generates language and image features to be used with pretrained image--language transformers.  It then allows you to use our released models to determine if an image-text pair match!\n",
        "\n",
        "This replicates our retrieval results in our TACL 2021 paper:\n",
        "\n",
        "[Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers](https://arxiv.org/abs/2102.00529)\n",
        "\n",
        "Paper Authors:  Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, and Aida Nematzadeh\n",
        "\n",
        "We also thank Sebastian Borgeaud and Cyprien de Masson d'Autume for their text preprocessing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_oXEX6F6_Rv"
      },
      "source": [
        "# Preproccessing Language and Images\n",
        "\n",
        "First, we use a detector to extract image features and SentencePiece to extract language tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmooLaXxzPS-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from io import BytesIO as StringIO\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kumubIq2g803"
      },
      "outputs": [],
      "source": [
        "# Install the tensorflow Object Detection API...\n",
        "# If you're running this offline, you also might need to install the protobuf-compiler:\n",
        "#   apt-get install protobuf-compiler\n",
        "\n",
        "! git clone -n https://github.com/tensorflow/models.git\n",
        "%cd models\n",
        "!git checkout 461b3587ef38b42cda151fa3b7d37706d77e4244\n",
        "%cd research\n",
        "! protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# Install TensorFlow Object Detection API\n",
        "%cp object_detection/packages/tf2/setup.py .\n",
        "! python -m pip install --upgrade pip\n",
        "! python -m pip install --use-feature=2020-resolver .\n",
        "\n",
        "# Test the installation\n",
        "! python object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyCSxln6hYDj"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.core import standard_fields as fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OCoLQ6msv0X"
      },
      "outputs": [],
      "source": [
        "!wget  https://storage.googleapis.com/dm-mmt-models/spiece.model -P '/tmp'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q9VtFoEJeMY"
      },
      "outputs": [],
      "source": [
        "features = {} # input to our model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiZXWgUln-nM"
      },
      "source": [
        "## Language Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6WQsSwvoCwv"
      },
      "source": [
        "### Helper Functions for Preprocessing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-l7ezeGlN3o"
      },
      "outputs": [],
      "source": [
        "SPIECE_UNDERLINE = '‚ñÅ'  # pylint: disable=invalid-encoded-data\n",
        "\n",
        "special_symbols = {\n",
        "    '\u003ccls\u003e': 3,\n",
        "    '\u003csep\u003e': 4,\n",
        "    '\u003cpad\u003e': 5,\n",
        "    '\u003cmask\u003e': 6,\n",
        "}\n",
        "CLS_ID = special_symbols['\u003ccls\u003e']\n",
        "SEP_ID = special_symbols['\u003csep\u003e']\n",
        "PAD_ID = special_symbols['\u003cpad\u003e']\n",
        "MASK_ID = special_symbols['\u003cmask\u003e']\n",
        "\n",
        "def is_start_piece(piece):\n",
        "  \"\"\"Returns True if the piece is a start piece for a word/symbol.\"\"\"\n",
        "  special_pieces = set(list('!\"#$%\u0026\\\"()*+,-./:;?@[\\\\]^_`{|}~'))\n",
        "  if piece.startswith(SPIECE_UNDERLINE):\n",
        "    return True\n",
        "  if piece.startswith('\u003c'):\n",
        "    return True\n",
        "  if piece in special_pieces:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def preprocess_text(inputs, lower=False, remove_space=True, keep_accents=False):\n",
        "  \"\"\"Preprocess the inputs.\"\"\"\n",
        "  if remove_space:\n",
        "    outputs = ' '.join(inputs.strip().split())\n",
        "  else:\n",
        "    outputs = inputs\n",
        "  outputs = outputs.replace('``', '\"').replace('\\'\\'', '\"')\n",
        "\n",
        "  if not keep_accents:\n",
        "    outputs = unicodedata.normalize('NFKD', outputs)\n",
        "    outputs = ''.join([c for c in outputs if not unicodedata.combining(c)])\n",
        "  if lower:\n",
        "    outputs = outputs.lower()\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def encode_pieces(sp_model, text, sample=False):\n",
        "  \"\"\"Encode the text to pieces using the given SentencePiece model sp_model.\"\"\"\n",
        "  if not sample:\n",
        "    pieces = sp_model.EncodeAsPieces(text)\n",
        "  else:\n",
        "    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "  new_pieces = []\n",
        "  for piece in pieces:\n",
        "    if len(piece) \u003e 1 and piece[-1] == ',' and piece[-2].isdigit():\n",
        "      cur_pieces = sp_model.EncodeAsPieces(\n",
        "          piece[:-1].replace(SPIECE_UNDERLINE, ''))\n",
        "      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "        if len(cur_pieces[0]) == 1:\n",
        "          cur_pieces = cur_pieces[1:]\n",
        "        else:\n",
        "          cur_pieces[0] = cur_pieces[0][1:]\n",
        "      cur_pieces.append(piece[-1])\n",
        "      new_pieces.extend(cur_pieces)\n",
        "    else:\n",
        "      new_pieces.append(piece)\n",
        "\n",
        "  return new_pieces\n",
        "\n",
        "\n",
        "def encode_ids(sp_model, text, sample=False):\n",
        "  pieces = encode_pieces(sp_model, text, sample=sample)\n",
        "  ids = [sp_model.PieceToId(piece) for piece in pieces]\n",
        "  return ids\n",
        "\n",
        "\n",
        "def tokens_to_word_indices(sp_model, tokens, offset=0):\n",
        "    \"\"\"Compute the word ids for the tokens.\n",
        "\n",
        "    The word indices start at offset, each time a new word is encountered, the\n",
        "    word id is increased by 1.\n",
        "\n",
        "    Args:\n",
        "      tokens: `list` of `int` SentencePiece tokens\n",
        "      offset: `int` start index\n",
        "\n",
        "    Returns:\n",
        "      A `list` of increasing integers. If element i and j are identical, then\n",
        "      tokens[i] and tokens[j] are part of the same word.\n",
        "    \"\"\"\n",
        "    word_indices = []\n",
        "    current_index = offset\n",
        "    for i, token in enumerate(tokens):\n",
        "      token_piece = sp_model.IdToPiece(token)\n",
        "      if i \u003e 0 and is_start_piece(token_piece):\n",
        "        current_index += 1\n",
        "      word_indices.append(current_index)\n",
        "\n",
        "    return word_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um98ejShoRNB"
      },
      "source": [
        "### Load the SentencePiece Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENAioP-BoOgZ"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as sp\n",
        "spm_path = '/tmp/spiece.model'\n",
        "spm = sp.SentencePieceProcessor()\n",
        "spm.Load(spm_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOIiWM_WoP5K"
      },
      "source": [
        "### Preprocessing Captions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA-h8RzDKRqT"
      },
      "outputs": [],
      "source": [
        "def create_sentence_features(seq_len, spm, captions, max_sentence_number=1):\n",
        "  def _add_sentence_pad():\n",
        "    pad_number = max_sentence_number - len(captions)\n",
        "    for _ in range(pad_number):\n",
        "      all_sents['tokens'] += [MASK_ID] * seq_len\n",
        "      all_sents['segment_ids'] += [0] * seq_len\n",
        "      all_sents['padding_mask'] += [1] * seq_len\n",
        "      all_sents['word_ids'] += [-2] * seq_len\n",
        "\n",
        "  # Limit the sentence length to seq_len\n",
        "  # We concatenate all sentences after checking the seq len and adding\n",
        "  # padding\n",
        "  all_sents = {}\n",
        "  for k in ['tokens', 'segment_ids', 'padding_mask', 'word_ids']:\n",
        "    all_sents[k] = []\n",
        "\n",
        "  for sentence in captions:\n",
        "    sentence = preprocess_text(sentence, remove_space=True, lower=True, keep_accents=False)\n",
        "\n",
        "    tokens = encode_ids(spm, sentence)\n",
        "    if len(tokens) \u003e= seq_len - 2:\n",
        "      tokens = tokens[:seq_len - 2]  # since we add two symbols\n",
        "\n",
        "    word_ids = tokens_to_word_indices(spm, tokens)\n",
        "    word_ids = ([-1] + word_ids + [-1])\n",
        "    # Need to create segment ids before adding special symbols to tokens\n",
        "    segment_ids = ([0] +  # SEP\n",
        "                    [0] * len(tokens) + [2]  # CLS\n",
        "                  )\n",
        "    tokens = ([SEP_ID] + tokens + [CLS_ID])\n",
        "    padding_mask = [0] * len(tokens)\n",
        "    # Note, we add padding at the start so that the last token is always [CLS]\n",
        "\n",
        "    if len(tokens) \u003c seq_len:\n",
        "      padding_len = seq_len - len(tokens)\n",
        "      tokens = [MASK_ID] * padding_len + tokens\n",
        "      \n",
        "      segment_ids = [0] * padding_len + segment_ids\n",
        "      padding_mask = [1] * padding_len + padding_mask\n",
        "      word_ids = [-2] * padding_len + word_ids\n",
        "\n",
        "\n",
        "    assert len(tokens) == seq_len\n",
        "    assert len(segment_ids) == seq_len\n",
        "    assert len(padding_mask) == seq_len\n",
        "    assert len(word_ids) == seq_len\n",
        "\n",
        "    all_sents['tokens'] += tokens\n",
        "    all_sents['segment_ids'] += segment_ids\n",
        "    all_sents['padding_mask'] += padding_mask\n",
        "    all_sents['word_ids'] += word_ids\n",
        "\n",
        "  # Add padding sentences to the end so that each example has\n",
        "  # max_sentence_number\n",
        "  if len(captions) \u003c max_sentence_number:\n",
        "    _add_sentence_pad()\n",
        "\n",
        "  return {\n",
        "      'text/token_ids': np.array(all_sents['tokens'], dtype=np.int32),\n",
        "      'text/segment_ids': np.array(all_sents['segment_ids'], dtype=np.int32),\n",
        "      'text/padding_mask': np.array(all_sents['padding_mask'], dtype=np.int32),\n",
        "      'text/word_ids': np.array(all_sents['word_ids'], dtype=np.int32),\n",
        "      'text/sentence_num': len(captions),\n",
        "  }\n",
        "         \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlV7HY-SLEj4"
      },
      "source": [
        "Get features for an example caption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXYlQ_rANxtZ"
      },
      "outputs": [],
      "source": [
        "features = create_sentence_features(seq_len=25, spm=spm, captions=['A man with a backpack holding a kitten.'])\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_uRSZXmodsj"
      },
      "source": [
        "## Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PczZB4Z4FvUt"
      },
      "source": [
        "###Load the Pretrained Object Detector "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xSz_GEZFoH0"
      },
      "outputs": [],
      "source": [
        "def LoadInferenceGraph(inference_graph_path):\n",
        "  \"\"\"Loads inference graph into tensorflow Graph object.\n",
        "\n",
        "  Args:\n",
        "    inference_graph_path: Path to inference graph.\n",
        "\n",
        "  Returns:\n",
        "    a tf.Graph object.\n",
        "  \"\"\"\n",
        "  od_graph = tf.Graph()\n",
        "  with od_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with open(inference_graph_path, 'rb') as fid:\n",
        "      serialized_graph = fid.read()\n",
        "      od_graph_def.ParseFromString(serialized_graph)\n",
        "      tf.import_graph_def(od_graph_def, name='')\n",
        "  return od_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoOK2n0ZLwVi"
      },
      "source": [
        "Download the pretrained object detector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZaA3ddA4-IW"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/dm-mmt-models/frozen_inference_graph.pb -P '/tmp'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qocil4KiP4K"
      },
      "outputs": [],
      "source": [
        "detection_graph = LoadInferenceGraph('/tmp/frozen_inference_graph.pb')\n",
        "print ('Successfully loaded frozen model from {}'.format('https://storage.googleapis.com/dm-mmt-models/frozen_inference_graph.pb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YkafsKflGkk"
      },
      "source": [
        "### Load an Example Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0hSW3qdmHXC"
      },
      "outputs": [],
      "source": [
        "def LoadImageIntoNumpyArray(path):\n",
        "\n",
        "  with open(path, 'rb') as img_file: \n",
        "    img = mpimg.imread(img_file)\n",
        "    (im_width, im_height) = img.shape[:2]\n",
        "    return img[:,:,:3].astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfAsIi_z5OY4"
      },
      "outputs": [],
      "source": [
        "# Download the image\n",
        "!wget --no-check-certificate https://storage.googleapis.com/dm-mmt-models/COCO_val2014_000000570107.jpeg -P '/tmp/' \n",
        "image_np = LoadImageIntoNumpyArray('/tmp/COCO_val2014_000000570107.jpeg')\n",
        "\n",
        "print('image type: %s' % str(image_np.dtype))\n",
        "print('image shape: %s' % str(image_np.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpVO4bCPPcrt"
      },
      "source": [
        "###Preprocessing Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r0qXvmzQDmG"
      },
      "source": [
        "Loading the object-label mappings for the dectector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Fa0dI14Z5W"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/dm-mmt-models/objatt_labelmap.txt -P '/tmp/' \n",
        "label_map_path = '/tmp/objatt_labelmap.txt'\n",
        "categories = label_map_util.create_categories_from_labelmap(label_map_path, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxWiy6hPlfy"
      },
      "source": [
        "Running inference on the object detector for a single image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7LeGadUzgeE"
      },
      "outputs": [],
      "source": [
        "def RunInferenceSingleImage(image, graph):\n",
        "  \"\"\"Run single image through tensorflow object detection graph.\n",
        "\n",
        "  This function runs an inference graph (frozen using the functions provided\n",
        "  in this file) on a (single) provided image and returns inference results in\n",
        "  numpy arrays.\n",
        "\n",
        "  Args:\n",
        "    image: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    graph: tensorflow graph object holding loaded model.  This graph can be\n",
        "      obtained by running the LoadInferenceGraph function above.\n",
        "\n",
        "  Returns:\n",
        "    output_dict: a dictionary holding the following entries:\n",
        "      `num_detections`: an integer\n",
        "      `detection_boxes`: a numpy (float32) array of shape [N, 4]\n",
        "      `detection_classes`: a numpy (uint8) array of shape [N]\n",
        "      `detection_scores`: a numpy (float32) array of shape [N]\n",
        "      `detection_masks`: a numpy (uint8) array of shape\n",
        "         [N, image_height, image_width] with values in {0, 1}\n",
        "      `detection_keypoints`: a numpy (float32) array of shape\n",
        "         [N, num_keypoints, 2]\n",
        "  \"\"\"\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      detection_fields = fields.DetectionResultFields\n",
        "      for key in [\n",
        "          v for k, v in vars(detection_fields).items()\n",
        "          if not k.startswith('__')\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "      if 'detection_keypoints' in output_dict:\n",
        "        output_dict['detection_keypoints'] = output_dict['detection_keypoints'][\n",
        "            0]\n",
        "  return output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdbzypy9MtLk"
      },
      "source": [
        "Pass an Image Through the Detector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e23DVGLlc6o"
      },
      "outputs": [],
      "source": [
        "# Run inference\n",
        "output_dict = RunInferenceSingleImage(image_np, detection_graph)\n",
        "output_dict['detection_features'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-e9VRV_8NI5"
      },
      "outputs": [],
      "source": [
        "output_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yni8oUuyQYuS"
      },
      "source": [
        "Preprocessing the output of the detector to be readable by our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyoXWQYKPQ91"
      },
      "outputs": [],
      "source": [
        "image_seq_num = 100 \n",
        "image_feat  = {}\n",
        "image_feat['height'] = image_np.shape[0]\n",
        "image_feat['width'] = image_np.shape[1]\n",
        "\n",
        "raw_feats = np.mean(np.mean(output_dict['detection_features'], axis=-2), axis=-2).squeeze() # image_feat['detection_features']\n",
        "num_detections = output_dict['num_detections']\n",
        "\n",
        "raw_scores = output_dict['detection_multiclass_scores'][:, :num_detections, ...].squeeze()\n",
        "\n",
        "# Find regions with highest class scores\n",
        "sorted_score_idxs = np.argsort(np.max(raw_scores[:, 1:], axis=-1))[::-1]\n",
        "\n",
        "# Collect features, boxes, and scores for highest scoring regions\n",
        "detection_feats = np.zeros((image_seq_num + 1, raw_feats.shape[-1]))\n",
        "detection_scores = np.zeros((image_seq_num + 1, raw_scores.shape[-1]))\n",
        "bbox_feats = np.zeros((image_seq_num + 1, 5))\n",
        "image_padding = np.ones((image_seq_num + 1,))\n",
        "padding_offset = max(image_seq_num + 1 - sorted_score_idxs.shape[0], 0)\n",
        "\n",
        "for i, index in enumerate(sorted_score_idxs[:image_seq_num]):\n",
        "  padded_index = i + padding_offset\n",
        "  detection_feats[padded_index, :] = raw_feats[index, :]\n",
        "  detection_scores[padded_index, :] = raw_scores[index, :]\n",
        "  # index 0 is 'background'\n",
        "  bbox_feats[padded_index, :4] = output_dict['detection_boxes'][index, :]\n",
        "  bbox_w = (output_dict['detection_boxes'][index, 3] -\n",
        "            output_dict['detection_boxes'][index, 1]) * image_feat['width']\n",
        "  bbox_h = (output_dict['detection_boxes'][index, 2] -\n",
        "            output_dict['detection_boxes'][index, 0]) * image_feat['height']\n",
        "  bbox_area = (bbox_w * bbox_h) / (image_feat['height'] * image_feat['width'])\n",
        "  bbox_feats[padded_index, -1] = bbox_area\n",
        "  image_padding[padded_index] = 0\n",
        "\n",
        "# Add in global image feature\n",
        "detection_feats[-1, :]= np.mean(detection_feats[padding_offset:-1, ...], axis=0).squeeze()\n",
        "bbox_feats[-1, :] = [0, 0, 1, 1, 1]\n",
        "image_padding[-1] = 0\n",
        "\n",
        "features.update(\n",
        "    {'image/bboxes': bbox_feats.astype(np.float32),\n",
        "     'image/padding_mask':  image_padding.astype(np.int32), \n",
        "     'image/detection_features': detection_feats.astype(np.float32),\n",
        "     'image/detection_scores': detection_scores.astype(np.float32)})               \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsS_lflb3lO5"
      },
      "outputs": [],
      "source": [
        "print(features['image/bboxes'].shape)\n",
        "print(features['image/detection_features'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgqt6SFBqEeH"
      },
      "source": [
        "### Visualizing the Detector Regions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bIAo59YyOu0"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "detection_classes = []\n",
        "detection_scores = []\n",
        "tuplet_index = {}\n",
        "\n",
        "for i in range(100):\n",
        "  raw_detection_scores_obj = output_dict['detection_multiclass_scores'][:,i,1:1600][0,:]\n",
        "  raw_detection_scores_att = output_dict['detection_multiclass_scores'][:,i,1600:][0,:]\n",
        "  max_obj = np.argmax(raw_detection_scores_obj)\n",
        "  max_att = np.argmax(raw_detection_scores_att)\n",
        "  tuplet_index[i] = {}\n",
        "  tuplet_index[i]['name'] = '%s %s' %(category_index[max_att+1600]['name'],\n",
        "                              category_index[max_obj+1]['name'])\n",
        "  detection_classes.append(i)\n",
        "  detection_scores.append(raw_detection_scores_obj[max_obj] +\n",
        "                          raw_detection_scores_att[max_att])\n",
        "\n",
        "# Create detections visualization\n",
        "bboxes = vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np.copy(),\n",
        "    output_dict['detection_boxes'],\n",
        "    np.array(detection_classes),\n",
        "    detection_scores,\n",
        "    tuplet_index,\n",
        "    instance_masks=None,\n",
        "    use_normalized_coordinates=True,\n",
        "    max_boxes_to_draw=15,\n",
        "    min_score_thresh=.05,\n",
        "    agnostic_mode=False)\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "_ = plt.imshow(bboxes)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nlv5_IyX53t"
      },
      "source": [
        "# Running Image-Text Pairs through the MMT\n",
        "\n",
        "Now that we have extracted our image and text features we can run them through our MMT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJpau4aEYIqO"
      },
      "source": [
        "## Use features extracted in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5q0zjE3oY9_"
      },
      "outputs": [],
      "source": [
        "# Select a model\n",
        "\n",
        "#@title Category-conditional sampling { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "tags = ['architecture-ft_image-q-12',\n",
        "        'architecture-ft_image-q-24',\n",
        "        'architecture-ft_language-q-12',\n",
        "        'architecture-ft_language-q-24',\n",
        "        'architecture-ft_single-modality',\n",
        "        'architecture-ft_single-stream',\n",
        "        'architecture_heads1-768',\n",
        "        'architecture_heads18-64',\n",
        "        'architecture_heads3-256',\n",
        "        'architecture_heads6-64',\n",
        "        'architecture_image-q-12',\n",
        "        'architecture_image-q-24',\n",
        "        'architecture_language-q-12',\n",
        "        'architecture_language-q-24',\n",
        "        'architecture_mixed-modality',\n",
        "        'architecture_single-modality',\n",
        "        'architecture_single-modality-hloss',\n",
        "        'architecture_single-stream',\n",
        "        'architecture_vilbert-12block',\n",
        "        'architecture_vilbert-1block',\n",
        "        'architecture_vilbert-2block',\n",
        "        'architecture_vilbert-4block',\n",
        "        'baseline-ft_baseline',\n",
        "        'baseline-ft_baseline-cls',\n",
        "        'baseline-ft_baseline-no-bert-transfer',\n",
        "        'baseline_baseline',\n",
        "        'baseline_baseline-cls',\n",
        "        'baseline_baseline-no-bert-transfer',\n",
        "        'data-ft_cc',\n",
        "        'data-ft_combined-dataset',\n",
        "        'data-ft_combined-instance',\n",
        "        'data-ft_mscoco',\n",
        "        'data-ft_mscoco-narratives',\n",
        "        'data-ft_oi-narratives',\n",
        "        'data-ft_sbu',\n",
        "        'data-ft_uniter-dataset',\n",
        "        'data-ft_uniter-instance',\n",
        "        'data-ft_vg',\n",
        "        'data_cc',\n",
        "        'data_cc-with-bert',\n",
        "        'data_combined-dataset',\n",
        "        'data_combined-instance',\n",
        "        'data_mscoco',\n",
        "        'data_mscoco-narratives',\n",
        "        'data_oi-narratives',\n",
        "        'data_sbu',\n",
        "        'data_uniter-dataset',\n",
        "        'data_uniter-instance',\n",
        "        'data_vg',\n",
        "        'loss_itm+mrm',\n",
        "        'loss_itm_mrm',\n",
        "        'loss_single-modality-contrastive1024',\n",
        "        'loss_single-modality-contrastive32',\n",
        "        'loss_v1-contrastive32',\n",
        "        'pixel_vilbert_cc-full-image']\n",
        "\n",
        "model = \"data_cc\" #@param [\"architecture-ft_image-q-12\", \"architecture-ft_image-q-24\", \"architecture-ft_language-q-12\", \"architecture-ft_language-q-24\", \"architecture-ft_single-modality\", \"architecture-ft_single-stream\", \"architecture_heads1-768\", \"architecture_heads18-64\", \"architecture_heads3-256\", \"architecture_heads6-64\", \"architecture_image-q-12\", \"architecture_image-q-24\", \"architecture_language-q-12\", \"architecture_language-q-24\", \"architecture_mixed-modality\", \"architecture_single-modality\", \"architecture_single-modality-hloss\", \"architecture_single-stream\", \"architecture_vilbert-12block\", \"architecture_vilbert-1block\", \"architecture_vilbert-2block\", \"architecture_vilbert-4block\", \"baseline-ft_baseline\", \"baseline-ft_baseline-cls\", \"baseline-ft_baseline-no-bert-transfer\", \"baseline_baseline\", \"baseline_baseline-cls\", \"baseline_baseline-no-bert-transfer\", \"data-ft_cc\", \"data-ft_combined-dataset\", \"data-ft_combined-instance\", \"data-ft_mscoco\", \"data-ft_mscoco-narratives\", \"data-ft_oi-narratives\", \"data-ft_sbu\", \"data-ft_uniter-dataset\", \"data-ft_uniter-instance\", \"data-ft_vg\", \"data_cc\", \"data_cc-with-bert\", \"data_combined-dataset\", \"data_combined-instance\", \"data_mscoco\", \"data_mscoco-narratives\", \"data_oi-narratives\", \"data_sbu\", \"data_uniter-dataset\", \"data_uniter-instance\", \"data_vg\", \"loss_itm+mrm\", \"loss_itm_mrm\", \"loss_single-modality-contrastive1024\", \"loss_single-modality-contrastive32\", \"loss_v1-contrastive32\"]\n",
        "\n",
        "tfhub_link = \"https://tfhub.dev/deepmind/mmt/%s/1\" %model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ugmFX5OYOfi"
      },
      "outputs": [],
      "source": [
        "model = hub.load(tfhub_link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhcy1XN3YvIw"
      },
      "outputs": [],
      "source": [
        "inputs={'image/bboxes': tf.expand_dims(features['image/bboxes'], 0),\n",
        "        'text/padding_mask': tf.expand_dims(features['text/padding_mask'], 0),\n",
        "        'image/padding_mask': tf.expand_dims(features['image/padding_mask'], 0),\n",
        "        'masked_tokens': tf.expand_dims(features['text/token_ids'], 0),\n",
        "        'text/segment_ids': tf.expand_dims(features['text/segment_ids'], 0),\n",
        "        'image/detection_features': tf.expand_dims(features['image/detection_features'], 0),\n",
        "        'text/token_ids': tf.expand_dims(features['text/token_ids'], 0)\n",
        "          }\n",
        "\n",
        "output = model.signatures['default'](**inputs)\n",
        "score = tf.nn.softmax(output['output']).numpy()[0]\n",
        "\n",
        "if score \u003e 0.5:\n",
        "  print('The text and image match!  (score: %0.03f)' %score)\n",
        "else: \n",
        "  print('The text and image do not match :( (score: %0.03f)' %score) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwlfRwMaVge6"
      },
      "source": [
        "# Running with Pre-Extracted Features\n",
        "\n",
        "We have pre-extracted MSCOCO and Flickr image features.  You can uset these pre-extracted features to do retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cEyRvb-YO8K"
      },
      "source": [
        "## Use Precomputed features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39CJERs_PJfd"
      },
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "\n",
        "!wget --no-check-certificate https://storage.googleapis.com/dm-mmt-models/features/coco_test/570107.pkl -P '/tmp/' \n",
        "with open('/tmp/570107.pkl', 'rb') as f:\n",
        "  im_feats = pkl.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_41h80NYH3o"
      },
      "outputs": [],
      "source": [
        "inputs={'image/bboxes': tf.expand_dims(features['image/bboxes'], 0),\n",
        "        'text/padding_mask': tf.expand_dims(features['text/padding_mask'], 0),\n",
        "        'image/padding_mask': tf.expand_dims(im_feats['image/padding_mask'], 0),\n",
        "        'masked_tokens': tf.expand_dims(features['text/token_ids'], 0),\n",
        "        'text/segment_ids': tf.expand_dims(features['text/segment_ids'], 0),\n",
        "        'image/detection_features': tf.expand_dims(im_feats['image/detection_features'], 0),\n",
        "        'text/token_ids': tf.expand_dims(features['text/token_ids'], 0)\n",
        "          }\n",
        "\n",
        "output = model.signatures['default'](**inputs)\n",
        "score = tf.nn.softmax(output['output']).numpy()[0]\n",
        "\n",
        "if score \u003e 0.5:\n",
        "  print('The text and image match!  (score: %0.03f)' %score)\n",
        "else: \n",
        "  print('The text and image do not match :( (score: %0.03f)' %score) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mmt_release.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1y-QoDky8RQTvM6shgfYguEJs8Bg4F3jg",
          "timestamp": 1635456767570
        },
        {
          "file_id": "1HEb0vz63HwqhY7vafLTetV3r4dAZ7iNV",
          "timestamp": 1628272609748
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
